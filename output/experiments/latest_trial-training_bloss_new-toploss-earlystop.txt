./spot_train_eval.sh latest_trial-training_bloss_new-toploss-earlystop.txt
{'dataset': {'name': 'anet', 'num_classes': 200, 'training': {'video_info_path': './data/activitynet_annotations/video_info_new.csv', 'video_info_path_unlabeled': './data/activitynet_annotations/', 'video_anno_path': './data/activitynet_annotations/anet_anno_action.json', 'num_frame': 16, 'output_path': '/root/models/SPOT/output/', 'unlabel_percent': 0.9, 'use_semi': True}, 'testing': {'video_info_path': './data/activitynet_annotations/video_info_new.csv', 'video_info_path_unlabeled': './data/activitynet_annotations/', 'video_anno_path': './data/activitynet_annotations/anet_anno_action.json', 'num_frame': 16, 'output_path': '/root/models/SPOT/output/', 'unlabel_percent': 0.9, 'use_semi': True}}, 'model': {'embedding_head': 4, 'feat_dim': 400, 'temporal_scale': 100}, 'pretraining': {'warmup_epoch': 9, 'consecutive_warmup_epochs': 3}, 'training': {'batch_size': 25, 'learning_rate': 0.0004, 'weight_decay': 0.005, 'alternate': True, 'max_epoch': 25, 'consecutive_train_epochs': 3, 'checkpoint_path': '/root/models/SPOT/output/', 'random_seed': 1, 'step': 10, 'gamma': 0.2, 'feature_path': '/data/i5O/ActivityNet1.3/train/', 'num_gpu': 1}, 'loss': {'lambda_1': 0.5, 'lambda_2': 0.4}, 'testing': {'feature_path': '/data/i5O/ActivityNet1.3/test/', 'cls_thresh': 0.01, 'mask_thresh': [0, 0.2, 0.4, 0.6, 0.8], 'class_thresh': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], 'top_k_snip': 10, 'top_k': 500, 'nms_thresh': 0.6}}

Total Number of Learnable Paramters (in M) :  6.67706
No of Gpus using to Train :  1 
 Saving all Checkpoints in path : /root/models/SPOT/output/
Loading train Video Information ...
  0% 0/9649 [00:00<?, ?it/s]  7% 698/9649 [00:00<00:01, 6969.27it/s] 15% 1419/9649 [00:00<00:01, 7108.59it/s] 22% 2142/9649 [00:00<00:01, 7158.25it/s] 30% 2858/9649 [00:00<00:00, 6877.75it/s] 37% 3548/9649 [00:00<00:00, 6801.96it/s] 44% 4230/9649 [00:00<00:00, 6423.36it/s] 51% 4876/9649 [00:00<00:00, 6349.76it/s] 57% 5514/9649 [00:00<00:00, 6203.02it/s] 64% 6136/9649 [00:00<00:00, 5991.62it/s] 70% 6737/9649 [00:01<00:00, 5823.07it/s] 76% 7321/9649 [00:01<00:00, 5467.71it/s] 82% 7872/9649 [00:01<00:00, 5240.26it/s] 87% 8399/9649 [00:01<00:00, 5045.83it/s] 92% 8906/9649 [00:01<00:00, 4743.13it/s] 98% 9409/9649 [00:01<00:00, 4819.70it/s]100% 9649/9649 [00:01<00:00, 5717.31it/s]
Loading train Video Information ...
  0% 0/9649 [00:00<?, ?it/s] 28% 2729/9649 [00:00<00:00, 27280.20it/s] 57% 5458/9649 [00:00<00:00, 27192.18it/s] 85% 8178/9649 [00:00<00:00, 27138.34it/s]100% 9649/9649 [00:00<00:00, 27126.15it/s]
Loading unlabel Video Information ...
  0% 0/8683 [00:00<?, ?it/s]  7% 627/8683 [00:00<00:01, 6265.88it/s] 14% 1254/8683 [00:00<00:01, 5974.97it/s] 21% 1853/8683 [00:00<00:01, 5835.37it/s] 28% 2438/8683 [00:00<00:01, 5665.92it/s] 35% 3006/8683 [00:00<00:01, 5456.71it/s] 41% 3553/8683 [00:00<00:00, 5317.52it/s] 47% 4086/8683 [00:00<00:00, 5175.58it/s] 53% 4605/8683 [00:00<00:00, 5002.71it/s] 59% 5106/8683 [00:00<00:00, 4800.53it/s] 64% 5588/8683 [00:01<00:00, 4284.04it/s] 69% 6026/8683 [00:01<00:00, 4227.38it/s] 74% 6455/8683 [00:01<00:00, 4156.58it/s] 79% 6875/8683 [00:01<00:00, 4101.19it/s] 84% 7288/8683 [00:01<00:00, 3996.90it/s] 89% 7690/8683 [00:01<00:00, 3916.09it/s] 93% 8083/8683 [00:01<00:00, 3831.12it/s] 98% 8467/8683 [00:01<00:00, 3732.12it/s]100% 8683/8683 [00:01<00:00, 4474.22it/s]
Loading validation Video Information ...
  0% 0/4728 [00:00<?, ?it/s] 10% 493/4728 [00:00<00:00, 4918.47it/s] 21% 985/4728 [00:00<00:00, 4701.42it/s] 31% 1456/4728 [00:00<00:00, 4568.36it/s] 40% 1914/4728 [00:00<00:00, 4468.91it/s] 50% 2362/4728 [00:00<00:00, 4433.17it/s] 59% 2806/4728 [00:00<00:00, 4259.79it/s] 68% 3233/4728 [00:00<00:00, 4094.49it/s] 77% 3644/4728 [00:00<00:00, 4056.04it/s] 86% 4051/4728 [00:00<00:00, 4059.04it/s] 94% 4458/4728 [00:01<00:00, 4022.52it/s]100% 4728/4728 [00:01<00:00, 4196.91it/s]0

n_iter  0 : loss (0.252529) + tot_loss (0.983506) + tot_loss_crop (0.929543) + loss_clip_order (0.755561) = final_loss = 2.921139
n_iter  1 : loss (0.241554) + tot_loss (1.005611) + tot_loss_crop (0.937873) + loss_clip_order (0.693142) = final_loss = 2.878179
n_iter  2 : loss (0.231432) + tot_loss (0.998161) + tot_loss_crop (0.931919) + loss_clip_order (0.693148) = final_loss = 2.854660
n_iter  3 : loss (0.222113) + tot_loss (0.991949) + tot_loss_crop (0.927820) + loss_clip_order (0.693148) = final_loss = 2.835030
n_iter  4 : loss (0.215389) + tot_loss (0.988719) + tot_loss_crop (0.923180) + loss_clip_order (0.693148) = final_loss = 2.820436
n_iter  5 : loss (0.203340) + tot_loss (0.992442) + tot_loss_crop (0.920453) + loss_clip_order (0.693148) = final_loss = 2.809383
n_iter  6 : loss (0.196261) + tot_loss (0.977814) + tot_loss_crop (0.910408) + loss_clip_order (0.693159) = final_loss = 2.777642
n_iter  7 : loss (0.192968) + tot_loss (0.952783) + tot_loss_crop (0.903655) + loss_clip_order (0.693148) = final_loss = 2.742553
n_iter  8 : loss (0.189596) + tot_loss (0.958285) + tot_loss_crop (0.895654) + loss_clip_order (0.693143) = final_loss = 2.736678
n_iter  9 : loss (0.177443) + tot_loss (0.938396) + tot_loss_crop (0.888093) + loss_clip_order (0.693198) = final_loss = 2.697131
n_iter 10 : loss (0.174950) + tot_loss (0.937938) + tot_loss_crop (0.880826) + loss_clip_order (0.693170) = final_loss = 2.686884
n_iter 11 : loss (0.176652) + tot_loss (0.913233) + tot_loss_crop (0.872527) + loss_clip_order (0.693211) = final_loss = 2.655623
n_iter 12 : loss (0.181630) + tot_loss (0.910183) + tot_loss_crop (0.863554) + loss_clip_order (0.693423) = final_loss = 2.648790
n_iter 13 : loss (0.181143) + tot_loss (0.899214) + tot_loss_crop (0.861622) + loss_clip_order (0.694077) = final_loss = 2.636056
n_iter 14 : loss (0.176667) + tot_loss (0.890630) + tot_loss_crop (0.858775) + loss_clip_order (0.695411) = final_loss = 2.621483
n_iter 15 : loss (0.187803) + tot_loss (0.884218) + tot_loss_crop (0.851833) + loss_clip_order (0.694535) = final_loss = 2.618387
n_iter 16 : loss (0.184758) + tot_loss (0.876718) + tot_loss_crop (0.850893) + loss_clip_order (0.695935) = final_loss = 2.608304
n_iter 17 : loss (0.179531) + tot_loss (0.874224) + tot_loss_crop (0.852918) + loss_clip_order (0.691698) = final_loss = 2.598371
n_iter 18 : loss (0.176625) + tot_loss (0.875174) + tot_loss_crop (0.851158) + loss_clip_order (0.696986) = final_loss = 2.599942
n_iter 19 : loss (0.170580) + tot_loss (0.861305) + tot_loss_crop (0.850032) + loss_clip_order (0.692247) = final_loss = 2.574163
n_iter 20 : loss (0.163980) + tot_loss (0.872579) + tot_loss_crop (0.852991) + loss_clip_order (0.694747) = final_loss = 2.584297
n_iter 21 : loss (0.157114) + tot_loss (0.890793) + tot_loss_crop (0.856468) + loss_clip_order (0.692453) = final_loss = 2.596828
n_iter 22 : loss (0.168926) + tot_loss (0.866134) + tot_loss_crop (0.845976) + loss_clip_order (0.694101) = final_loss = 2.575137
n_iter 23 : loss (0.168898) + tot_loss (0.866382) + tot_loss_crop (0.849623) + loss_clip_order (0.697737) = final_loss = 2.582641
n_iter 24 : loss (0.168229) + tot_loss (0.850841) + tot_loss_crop (0.845356) + loss_clip_order (0.693207) = final_loss = 2.557633
n_iter 25 : loss (0.173951) + tot_loss (0.852493) + tot_loss_crop (0.839325) + loss_clip_order (0.691494) = final_loss = 2.557263
n_iter 26 : loss (0.166179) + tot_loss (0.858501) + tot_loss_crop (0.847144) + loss_clip_order (0.694068) = final_loss = 2.565892
n_iter 27 : loss (0.161427) + tot_loss (0.860013) + tot_loss_crop (0.847117) + loss_clip_order (0.691884) = final_loss = 2.560441
n_iter 28 : loss (0.161805) + tot_loss (0.833971) + tot_loss_crop (0.842526) + loss_clip_order (0.693448) = final_loss = 2.531749
n_iter 29 : loss (0.167256) + tot_loss (0.860881) + tot_loss_crop (0.842541) + loss_clip_order (0.692754) = final_loss = 2.563432
n_iter 30 : loss (0.160428) + tot_loss (0.855029) + tot_loss_crop (0.842063) + loss_clip_order (0.692338) = final_loss = 2.549858
[Pretraining Epoch 000] Total-Loss 0.86 =  F-Loss 0.86 + Clip-Loss 0.69 (train)
/root/models/venv_SPOT/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
n_iter  0 : loss (0.167395) + tot_loss (0.842994) + tot_loss_crop (0.839080) + loss_clip_order (0.692410) = final_loss = 2.541878
n_iter  1 : loss (0.174158) + tot_loss (0.860874) + tot_loss_crop (0.835809) + loss_clip_order (0.691240) = final_loss = 2.562081
n_iter  2 : loss (0.174538) + tot_loss (0.845117) + tot_loss_crop (0.836719) + loss_clip_order (0.693101) = final_loss = 2.549475
n_iter  3 : loss (0.175960) + tot_loss (0.834615) + tot_loss_crop (0.831309) + loss_clip_order (0.691785) = final_loss = 2.533669
n_iter  4 : loss (0.171403) + tot_loss (0.828600) + tot_loss_crop (0.833676) + loss_clip_order (0.692926) = final_loss = 2.526605
n_iter  5 : loss (0.167790) + tot_loss (0.832220) + tot_loss_crop (0.830843) + loss_clip_order (0.691382) = final_loss = 2.522234
n_iter  6 : loss (0.160943) + tot_loss (0.830060) + tot_loss_crop (0.833343) + loss_clip_order (0.695473) = final_loss = 2.519819
n_iter  7 : loss (0.156528) + tot_loss (0.810494) + tot_loss_crop (0.831966) + loss_clip_order (0.691759) = final_loss = 2.490747
n_iter  8 : loss (0.163434) + tot_loss (0.823938) + tot_loss_crop (0.833923) + loss_clip_order (0.693764) = final_loss = 2.515058
n_iter  9 : loss (0.169686) + tot_loss (0.813363) + tot_loss_crop (0.830103) + loss_clip_order (0.690050) = final_loss = 2.503202
n_iter 10 : loss (0.167256) + tot_loss (0.826302) + tot_loss_crop (0.829322) + loss_clip_order (0.688990) = final_loss = 2.511869
n_iter 11 : loss (0.176516) + tot_loss (0.808947) + tot_loss_crop (0.821429) + loss_clip_order (0.678420) = final_loss = 2.485312
n_iter 12 : loss (0.183544) + tot_loss (0.818583) + tot_loss_crop (0.826173) + loss_clip_order (0.667648) = final_loss = 2.495948
n_iter 13 : loss (0.194964) + tot_loss (0.819989) + tot_loss_crop (0.824212) + loss_clip_order (0.653313) = final_loss = 2.492478
n_iter 14 : loss (0.173409) + tot_loss (0.827335) + tot_loss_crop (0.823317) + loss_clip_order (0.689724) = final_loss = 2.513785
n_iter 15 : loss (0.162399) + tot_loss (0.834184) + tot_loss_crop (0.830058) + loss_clip_order (0.691790) = final_loss = 2.518431
n_iter 16 : loss (0.177349) + tot_loss (0.838177) + tot_loss_crop (0.833599) + loss_clip_order (0.692347) = final_loss = 2.541472
n_iter 17 : loss (0.165506) + tot_loss (0.838871) + tot_loss_crop (0.836370) + loss_clip_order (0.691475) = final_loss = 2.532222
n_iter 18 : loss (0.170303) + tot_loss (0.837304) + tot_loss_crop (0.831817) + loss_clip_order (0.691723) = final_loss = 2.531147
n_iter 19 : loss (0.175105) + tot_loss (0.819892) + tot_loss_crop (0.822780) + loss_clip_order (0.693191) = final_loss = 2.510968
n_iter 20 : loss (0.164585) + tot_loss (0.824551) + tot_loss_crop (0.827756) + loss_clip_order (0.693027) = final_loss = 2.509919
n_iter 21 : loss (0.169813) + tot_loss (0.837729) + tot_loss_crop (0.822523) + loss_clip_order (0.692799) = final_loss = 2.522864
n_iter 22 : loss (0.170439) + tot_loss (0.812129) + tot_loss_crop (0.823558) + loss_clip_order (0.690775) = final_loss = 2.496902
n_iter 23 : loss (0.170403) + tot_loss (0.810881) + tot_loss_crop (0.825750) + loss_clip_order (0.689515) = final_loss = 2.496549
n_iter 24 : loss (0.173278) + tot_loss (0.797270) + tot_loss_crop (0.816842) + loss_clip_order (0.682746) = final_loss = 2.470136
n_iter 25 : loss (0.169787) + tot_loss (0.800169) + tot_loss_crop (0.814673) + loss_clip_order (0.682336) = final_loss = 2.466965
n_iter 26 : loss (0.168021) + tot_loss (0.806788) + tot_loss_crop (0.817839) + loss_clip_order (0.669349) = final_loss = 2.461997
n_iter 27 : loss (0.168966) + tot_loss (0.810847) + tot_loss_crop (0.814311) + loss_clip_order (0.646914) = final_loss = 2.441038
n_iter 28 : loss (0.177940) + tot_loss (0.788691) + tot_loss_crop (0.813052) + loss_clip_order (0.601561) = final_loss = 2.381244
n_iter 29 : loss (0.165724) + tot_loss (0.814928) + tot_loss_crop (0.820514) + loss_clip_order (0.594535) = final_loss = 2.395701
n_iter 30 : loss (0.167537) + tot_loss (0.809637) + tot_loss_crop (0.815279) + loss_clip_order (0.597142) = final_loss = 2.389596
[Pretraining Epoch 001] Total-Loss 0.81 =  F-Loss 0.81 + Clip-Loss 0.60 (train)
n_iter  0 : loss (0.171930) + tot_loss (0.799749) + tot_loss_crop (0.810356) + loss_clip_order (0.574459) = final_loss = 2.356493
n_iter  1 : loss (0.168945) + tot_loss (0.818749) + tot_loss_crop (0.817664) + loss_clip_order (0.594506) = final_loss = 2.399865
n_iter  2 : loss (0.161330) + tot_loss (0.804842) + tot_loss_crop (0.810585) + loss_clip_order (0.589858) = final_loss = 2.366616
n_iter  3 : loss (0.157471) + tot_loss (0.795563) + tot_loss_crop (0.811443) + loss_clip_order (0.601735) = final_loss = 2.366212
n_iter  4 : loss (0.167287) + tot_loss (0.788163) + tot_loss_crop (0.805551) + loss_clip_order (0.575252) = final_loss = 2.336253
n_iter  5 : loss (0.179655) + tot_loss (0.788215) + tot_loss_crop (0.797841) + loss_clip_order (0.544639) = final_loss = 2.310349
n_iter  6 : loss (0.182931) + tot_loss (0.786362) + tot_loss_crop (0.808328) + loss_clip_order (0.641598) = final_loss = 2.419219
n_iter  7 : loss (0.169695) + tot_loss (0.767495) + tot_loss_crop (0.799629) + loss_clip_order (0.560479) = final_loss = 2.297299
n_iter  8 : loss (0.165589) + tot_loss (0.781676) + tot_loss_crop (0.799435) + loss_clip_order (0.616594) = final_loss = 2.363293
n_iter  9 : loss (0.165718) + tot_loss (0.775733) + tot_loss_crop (0.797938) + loss_clip_order (0.628009) = final_loss = 2.367398
n_iter 10 : loss (0.164267) + tot_loss (0.789323) + tot_loss_crop (0.797702) + loss_clip_order (0.637261) = final_loss = 2.388553
n_iter 11 : loss (0.163932) + tot_loss (0.773009) + tot_loss_crop (0.796472) + loss_clip_order (0.609572) = final_loss = 2.342985
n_iter 12 : loss (0.170487) + tot_loss (0.781835) + tot_loss_crop (0.792824) + loss_clip_order (0.571211) = final_loss = 2.316358
n_iter 13 : loss (0.167142) + tot_loss (0.778994) + tot_loss_crop (0.801956) + loss_clip_order (0.516497) = final_loss = 2.264590
n_iter 14 : loss (0.182924) + tot_loss (0.783247) + tot_loss_crop (0.802198) + loss_clip_order (0.631125) = final_loss = 2.399493
n_iter 15 : loss (0.172343) + tot_loss (0.777618) + tot_loss_crop (0.788935) + loss_clip_order (0.527956) = final_loss = 2.266853
n_iter 16 : loss (0.166111) + tot_loss (0.776533) + tot_loss_crop (0.789871) + loss_clip_order (0.572748) = final_loss = 2.305262
n_iter 17 : loss (0.167026) + tot_loss (0.777925) + tot_loss_crop (0.789922) + loss_clip_order (0.576400) = final_loss = 2.311273
n_iter 18 : loss (0.167382) + tot_loss (0.779050) + tot_loss_crop (0.788455) + loss_clip_order (0.580056) = final_loss = 2.314943
n_iter 19 : loss (0.176588) + tot_loss (0.765955) + tot_loss_crop (0.779911) + loss_clip_order (0.580821) = final_loss = 2.303275
n_iter 20 : loss (0.165725) + tot_loss (0.772908) + tot_loss_crop (0.785979) + loss_clip_order (0.529954) = final_loss = 2.254566
n_iter 21 : loss (0.152655) + tot_loss (0.788680) + tot_loss_crop (0.794475) + loss_clip_order (0.499689) = final_loss = 2.235500
n_iter 22 : loss (0.174785) + tot_loss (0.766374) + tot_loss_crop (0.781598) + loss_clip_order (0.471015) = final_loss = 2.193772
n_iter 23 : loss (0.166139) + tot_loss (0.767111) + tot_loss_crop (0.793590) + loss_clip_order (0.419481) = final_loss = 2.146320
n_iter 24 : loss (0.174327) + tot_loss (0.757509) + tot_loss_crop (0.791755) + loss_clip_order (0.494219) = final_loss = 2.217810
n_iter 25 : loss (0.172253) + tot_loss (0.759644) + tot_loss_crop (0.781576) + loss_clip_order (0.429714) = final_loss = 2.143187
n_iter 26 : loss (0.166182) + tot_loss (0.765628) + tot_loss_crop (0.781592) + loss_clip_order (0.430566) = final_loss = 2.143968
n_iter 27 : loss (0.160817) + tot_loss (0.770695) + tot_loss_crop (0.786516) + loss_clip_order (0.433709) = final_loss = 2.151737
n_iter 28 : loss (0.174861) + tot_loss (0.748129) + tot_loss_crop (0.775852) + loss_clip_order (0.432998) = final_loss = 2.131839
n_iter 29 : loss (0.154207) + tot_loss (0.773124) + tot_loss_crop (0.785411) + loss_clip_order (0.432992) = final_loss = 2.145734
n_iter 30 : loss (0.155294) + tot_loss (0.768287) + tot_loss_crop (0.782585) + loss_clip_order (0.425841) = final_loss = 2.132006
[Pretraining Epoch 002] Total-Loss 0.77 =  F-Loss 0.77 + Clip-Loss 0.43 (train)
training epoch 0
use Semi !!!
[Iteration 000] Total-Loss 6.18 = T-Loss 5.46 + B-Loss 0.72 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 5.21 = T-Loss 4.52 + B-Loss 0.69 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 5.13 = T-Loss 4.45 + B-Loss 0.68 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 5.14 = T-Loss 4.46 + B-Loss 0.68 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 000] Total-Loss 5.14 = T-Loss 4.46 + B-Loss 0.68 (train)[0m
[Epoch 000] Total-Loss 4.99 = T-Loss 4.33 + B-Loss 0.65  (val)
training epoch 1
use Semi !!!
[Iteration 000] Total-Loss 4.58 = T-Loss 3.89 + B-Loss 0.69 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 4.72 = T-Loss 4.06 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 4.72 = T-Loss 4.06 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 4.75 = T-Loss 4.09 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 001] Total-Loss 4.75 = T-Loss 4.09 + B-Loss 0.66 (train)[0m
[Epoch 001] Total-Loss 4.65 = T-Loss 4.02 + B-Loss 0.64  (val)
training epoch 2
use Semi !!!
[Iteration 000] Total-Loss 3.93 = T-Loss 3.26 + B-Loss 0.68 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 4.20 = T-Loss 3.55 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 4.17 = T-Loss 3.53 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 4.11 = T-Loss 3.46 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 002] Total-Loss 4.11 = T-Loss 3.46 + B-Loss 0.65 (train)[0m
[Epoch 002] Total-Loss 4.08 = T-Loss 3.46 + B-Loss 0.63  (val)
3
n_iter  0 : loss (0.226173) + tot_loss (0.718731) + tot_loss_crop (0.749079) + loss_clip_order (0.634018) = final_loss = 2.328001
n_iter  1 : loss (0.221580) + tot_loss (0.740275) + tot_loss_crop (0.748162) + loss_clip_order (0.554474) = final_loss = 2.264491
n_iter  2 : loss (0.212895) + tot_loss (0.732519) + tot_loss_crop (0.749621) + loss_clip_order (0.609477) = final_loss = 2.304511
n_iter  3 : loss (0.202761) + tot_loss (0.729022) + tot_loss_crop (0.750567) + loss_clip_order (0.616461) = final_loss = 2.298811
n_iter  4 : loss (0.189635) + tot_loss (0.727031) + tot_loss_crop (0.753639) + loss_clip_order (0.621423) = final_loss = 2.291727
n_iter  5 : loss (0.187269) + tot_loss (0.730648) + tot_loss_crop (0.753400) + loss_clip_order (0.599035) = final_loss = 2.270352
n_iter  6 : loss (0.174742) + tot_loss (0.727852) + tot_loss_crop (0.751632) + loss_clip_order (0.574308) = final_loss = 2.228534
n_iter  7 : loss (0.170114) + tot_loss (0.711643) + tot_loss_crop (0.750454) + loss_clip_order (0.489161) = final_loss = 2.121372
n_iter  8 : loss (0.176102) + tot_loss (0.727413) + tot_loss_crop (0.754865) + loss_clip_order (1.228167) = final_loss = 2.886546
n_iter  9 : loss (0.168799) + tot_loss (0.727978) + tot_loss_crop (0.747946) + loss_clip_order (0.624608) = final_loss = 2.269330
n_iter 10 : loss (0.158942) + tot_loss (0.764954) + tot_loss_crop (0.763750) + loss_clip_order (0.673679) = final_loss = 2.361326
n_iter 11 : loss (0.169829) + tot_loss (0.774018) + tot_loss_crop (0.766270) + loss_clip_order (0.679224) = final_loss = 2.389341
n_iter 12 : loss (0.157078) + tot_loss (0.799108) + tot_loss_crop (0.773731) + loss_clip_order (0.687998) = final_loss = 2.417915
n_iter 13 : loss (0.156183) + tot_loss (0.806320) + tot_loss_crop (0.779620) + loss_clip_order (0.690199) = final_loss = 2.432322
n_iter 14 : loss (0.172591) + tot_loss (0.807930) + tot_loss_crop (0.776604) + loss_clip_order (0.690544) = final_loss = 2.447669
n_iter 15 : loss (0.154308) + tot_loss (0.798569) + tot_loss_crop (0.773110) + loss_clip_order (0.689255) = final_loss = 2.415242
n_iter 16 : loss (0.157604) + tot_loss (0.788169) + tot_loss_crop (0.766431) + loss_clip_order (0.686585) = final_loss = 2.398790
n_iter 17 : loss (0.160557) + tot_loss (0.771282) + tot_loss_crop (0.759595) + loss_clip_order (0.681320) = final_loss = 2.372754
n_iter 18 : loss (0.166694) + tot_loss (0.752501) + tot_loss_crop (0.751267) + loss_clip_order (0.671485) = final_loss = 2.341948
n_iter 19 : loss (0.171478) + tot_loss (0.725287) + tot_loss_crop (0.746496) + loss_clip_order (0.601981) = final_loss = 2.245243
n_iter 20 : loss (0.200742) + tot_loss (0.734367) + tot_loss_crop (0.748415) + loss_clip_order (0.882041) = final_loss = 2.565566
n_iter 21 : loss (0.149105) + tot_loss (0.796628) + tot_loss_crop (0.754648) + loss_clip_order (0.679138) = final_loss = 2.379520
n_iter 22 : loss (0.174684) + tot_loss (0.838662) + tot_loss_crop (0.776678) + loss_clip_order (0.692493) = final_loss = 2.482517
n_iter 23 : loss (0.158031) + tot_loss (0.883356) + tot_loss_crop (0.796211) + loss_clip_order (0.693425) = final_loss = 2.531022
n_iter 24 : loss (0.160993) + tot_loss (0.885315) + tot_loss_crop (0.804163) + loss_clip_order (0.693546) = final_loss = 2.544017
n_iter 25 : loss (0.168111) + tot_loss (0.909261) + tot_loss_crop (0.813319) + loss_clip_order (0.693746) = final_loss = 2.584437
n_iter 26 : loss (0.156690) + tot_loss (0.917078) + tot_loss_crop (0.817971) + loss_clip_order (0.693743) = final_loss = 2.585482
n_iter 27 : loss (0.172076) + tot_loss (0.925130) + tot_loss_crop (0.820466) + loss_clip_order (0.693755) = final_loss = 2.611428
n_iter 28 : loss (0.160239) + tot_loss (0.912769) + tot_loss_crop (0.813832) + loss_clip_order (0.693771) = final_loss = 2.580611
n_iter 29 : loss (0.172245) + tot_loss (0.930858) + tot_loss_crop (0.817325) + loss_clip_order (0.693761) = final_loss = 2.614189
n_iter 30 : loss (0.167911) + tot_loss (0.932393) + tot_loss_crop (0.810415) + loss_clip_order (0.693763) = final_loss = 2.604483
[Pretraining Epoch 003] Total-Loss 0.93 =  F-Loss 0.93 + Clip-Loss 0.69 (train)
n_iter  0 : loss (0.163648) + tot_loss (0.923543) + tot_loss_crop (0.806880) + loss_clip_order (0.693791) = final_loss = 2.587862
n_iter  1 : loss (0.165928) + tot_loss (0.939561) + tot_loss_crop (0.805534) + loss_clip_order (0.693423) = final_loss = 2.604445
n_iter  2 : loss (0.160490) + tot_loss (0.931054) + tot_loss_crop (0.796569) + loss_clip_order (0.693263) = final_loss = 2.581377
n_iter  3 : loss (0.162764) + tot_loss (0.923969) + tot_loss_crop (0.792081) + loss_clip_order (0.693463) = final_loss = 2.572277
n_iter  4 : loss (0.151385) + tot_loss (0.923270) + tot_loss_crop (0.783133) + loss_clip_order (0.692781) = final_loss = 2.550569
n_iter  5 : loss (0.147377) + tot_loss (0.931092) + tot_loss_crop (0.774459) + loss_clip_order (0.691441) = final_loss = 2.544370
n_iter  6 : loss (0.147363) + tot_loss (0.920672) + tot_loss_crop (0.763916) + loss_clip_order (0.686854) = final_loss = 2.518805
n_iter  7 : loss (0.157795) + tot_loss (0.904574) + tot_loss_crop (0.759292) + loss_clip_order (0.686649) = final_loss = 2.508311
n_iter  8 : loss (0.161000) + tot_loss (0.915521) + tot_loss_crop (0.751118) + loss_clip_order (0.671365) = final_loss = 2.499003
n_iter  9 : loss (0.152099) + tot_loss (0.907102) + tot_loss_crop (0.742455) + loss_clip_order (0.654066) = final_loss = 2.455722
n_iter 10 : loss (0.167194) + tot_loss (0.915909) + tot_loss_crop (0.731400) + loss_clip_order (0.631955) = final_loss = 2.446458
n_iter 11 : loss (0.172385) + tot_loss (0.907852) + tot_loss_crop (0.724353) + loss_clip_order (0.607593) = final_loss = 2.412182
n_iter 12 : loss (0.169209) + tot_loss (0.914335) + tot_loss_crop (0.720493) + loss_clip_order (0.585769) = final_loss = 2.389807
n_iter 13 : loss (0.161986) + tot_loss (0.912720) + tot_loss_crop (0.720186) + loss_clip_order (0.552706) = final_loss = 2.347598
n_iter 14 : loss (0.143399) + tot_loss (0.912118) + tot_loss_crop (0.723987) + loss_clip_order (0.545636) = final_loss = 2.325140
n_iter 15 : loss (0.165158) + tot_loss (0.904454) + tot_loss_crop (0.715701) + loss_clip_order (0.527452) = final_loss = 2.312765
n_iter 16 : loss (0.168879) + tot_loss (0.904773) + tot_loss_crop (0.709972) + loss_clip_order (0.519067) = final_loss = 2.302690
n_iter 17 : loss (0.154337) + tot_loss (0.896009) + tot_loss_crop (0.713321) + loss_clip_order (0.514241) = final_loss = 2.277909
n_iter 18 : loss (0.160792) + tot_loss (0.896295) + tot_loss_crop (0.707821) + loss_clip_order (0.480809) = final_loss = 2.245717
n_iter 19 : loss (0.166860) + tot_loss (0.876072) + tot_loss_crop (0.703426) + loss_clip_order (0.507504) = final_loss = 2.253862
n_iter 20 : loss (0.166906) + tot_loss (0.884293) + tot_loss_crop (0.702169) + loss_clip_order (0.483182) = final_loss = 2.236550
n_iter 21 : loss (0.159254) + tot_loss (0.905387) + tot_loss_crop (0.705093) + loss_clip_order (0.458226) = final_loss = 2.227960
n_iter 22 : loss (0.167796) + tot_loss (0.878175) + tot_loss_crop (0.698503) + loss_clip_order (0.453891) = final_loss = 2.198365
n_iter 23 : loss (0.150259) + tot_loss (0.887061) + tot_loss_crop (0.708444) + loss_clip_order (0.456128) = final_loss = 2.201892
n_iter 24 : loss (0.147628) + tot_loss (0.865323) + tot_loss_crop (0.704508) + loss_clip_order (0.453674) = final_loss = 2.171133
n_iter 25 : loss (0.166854) + tot_loss (0.871812) + tot_loss_crop (0.696132) + loss_clip_order (0.433814) = final_loss = 2.168612
n_iter 26 : loss (0.156715) + tot_loss (0.868945) + tot_loss_crop (0.699744) + loss_clip_order (0.426144) = final_loss = 2.151549
n_iter 27 : loss (0.156449) + tot_loss (0.874207) + tot_loss_crop (0.698789) + loss_clip_order (0.428009) = final_loss = 2.157454
n_iter 28 : loss (0.164824) + tot_loss (0.853954) + tot_loss_crop (0.693819) + loss_clip_order (0.426786) = final_loss = 2.139383
n_iter 29 : loss (0.155732) + tot_loss (0.869578) + tot_loss_crop (0.699472) + loss_clip_order (0.421962) = final_loss = 2.146744
n_iter 30 : loss (0.158003) + tot_loss (0.870464) + tot_loss_crop (0.696289) + loss_clip_order (0.421909) = final_loss = 2.146665
[Pretraining Epoch 004] Total-Loss 0.87 =  F-Loss 0.87 + Clip-Loss 0.42 (train)
n_iter  0 : loss (0.161689) + tot_loss (0.857112) + tot_loss_crop (0.692985) + loss_clip_order (0.413725) = final_loss = 2.125511
n_iter  1 : loss (0.168412) + tot_loss (0.870104) + tot_loss_crop (0.690361) + loss_clip_order (0.405266) = final_loss = 2.134144
n_iter  2 : loss (0.159756) + tot_loss (0.860653) + tot_loss_crop (0.690268) + loss_clip_order (0.412570) = final_loss = 2.123248
n_iter  3 : loss (0.161430) + tot_loss (0.849851) + tot_loss_crop (0.689650) + loss_clip_order (0.409190) = final_loss = 2.110121
n_iter  4 : loss (0.170050) + tot_loss (0.850842) + tot_loss_crop (0.681243) + loss_clip_order (0.404480) = final_loss = 2.106614
n_iter  5 : loss (0.156990) + tot_loss (0.856207) + tot_loss_crop (0.687912) + loss_clip_order (0.394790) = final_loss = 2.095898
n_iter  6 : loss (0.155299) + tot_loss (0.846899) + tot_loss_crop (0.685060) + loss_clip_order (0.403976) = final_loss = 2.091234
n_iter  7 : loss (0.165103) + tot_loss (0.831094) + tot_loss_crop (0.683875) + loss_clip_order (0.411079) = final_loss = 2.091151
n_iter  8 : loss (0.157989) + tot_loss (0.841163) + tot_loss_crop (0.680518) + loss_clip_order (0.401702) = final_loss = 2.081372
n_iter  9 : loss (0.167978) + tot_loss (0.831046) + tot_loss_crop (0.679073) + loss_clip_order (0.410520) = final_loss = 2.088617
n_iter 10 : loss (0.162838) + tot_loss (0.841514) + tot_loss_crop (0.678538) + loss_clip_order (0.394078) = final_loss = 2.076968
n_iter 11 : loss (0.162424) + tot_loss (0.835313) + tot_loss_crop (0.677923) + loss_clip_order (0.404725) = final_loss = 2.080385
n_iter 12 : loss (0.151205) + tot_loss (0.838342) + tot_loss_crop (0.681109) + loss_clip_order (0.404566) = final_loss = 2.075222
n_iter 13 : loss (0.162716) + tot_loss (0.839677) + tot_loss_crop (0.674501) + loss_clip_order (0.384726) = final_loss = 2.061620
n_iter 14 : loss (0.163045) + tot_loss (0.839114) + tot_loss_crop (0.674350) + loss_clip_order (0.382422) = final_loss = 2.058930
n_iter 15 : loss (0.155796) + tot_loss (0.832623) + tot_loss_crop (0.677078) + loss_clip_order (0.375326) = final_loss = 2.040823
n_iter 16 : loss (0.156684) + tot_loss (0.833105) + tot_loss_crop (0.675513) + loss_clip_order (0.378731) = final_loss = 2.044033
n_iter 17 : loss (0.166643) + tot_loss (0.824787) + tot_loss_crop (0.670034) + loss_clip_order (0.383642) = final_loss = 2.045106
n_iter 18 : loss (0.149466) + tot_loss (0.825881) + tot_loss_crop (0.675601) + loss_clip_order (0.379597) = final_loss = 2.030545
n_iter 19 : loss (0.153985) + tot_loss (0.805943) + tot_loss_crop (0.673177) + loss_clip_order (0.404162) = final_loss = 2.037266
n_iter 20 : loss (0.153143) + tot_loss (0.815390) + tot_loss_crop (0.670129) + loss_clip_order (0.382156) = final_loss = 2.020819
n_iter 21 : loss (0.159884) + tot_loss (0.836269) + tot_loss_crop (0.668427) + loss_clip_order (0.369400) = final_loss = 2.033980
n_iter 22 : loss (0.160987) + tot_loss (0.809404) + tot_loss_crop (0.668327) + loss_clip_order (0.373971) = final_loss = 2.012690
n_iter 23 : loss (0.158636) + tot_loss (0.818030) + tot_loss_crop (0.668699) + loss_clip_order (0.369617) = final_loss = 2.014982
n_iter 24 : loss (0.158511) + tot_loss (0.796702) + tot_loss_crop (0.664808) + loss_clip_order (0.369405) = final_loss = 1.989427
n_iter 25 : loss (0.154924) + tot_loss (0.804158) + tot_loss_crop (0.665445) + loss_clip_order (0.356418) = final_loss = 1.980945
n_iter 26 : loss (0.159448) + tot_loss (0.800594) + tot_loss_crop (0.664156) + loss_clip_order (0.354567) = final_loss = 1.978766
n_iter 27 : loss (0.164494) + tot_loss (0.805132) + tot_loss_crop (0.660574) + loss_clip_order (0.359845) = final_loss = 1.990045
n_iter 28 : loss (0.160788) + tot_loss (0.784872) + tot_loss_crop (0.658444) + loss_clip_order (0.365811) = final_loss = 1.969914
n_iter 29 : loss (0.153168) + tot_loss (0.799531) + tot_loss_crop (0.663694) + loss_clip_order (0.358644) = final_loss = 1.975038
n_iter 30 : loss (0.153848) + tot_loss (0.798482) + tot_loss_crop (0.660873) + loss_clip_order (0.363686) = final_loss = 1.976888
[Pretraining Epoch 005] Total-Loss 0.80 =  F-Loss 0.80 + Clip-Loss 0.36 (train)
training epoch 3
use Semi !!!
[Iteration 000] Total-Loss 4.24 = T-Loss 3.54 + B-Loss 0.70 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 4.54 = T-Loss 3.86 + B-Loss 0.68 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 4.49 = T-Loss 3.81 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 4.40 = T-Loss 3.73 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 003] Total-Loss 4.40 = T-Loss 3.73 + B-Loss 0.67 (train)[0m
[Epoch 003] Total-Loss 4.42 = T-Loss 3.79 + B-Loss 0.63  (val)
training epoch 4
use Semi !!!
[Iteration 000] Total-Loss 3.54 = T-Loss 2.88 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.74 = T-Loss 3.10 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.73 = T-Loss 3.09 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.75 = T-Loss 3.11 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 004] Total-Loss 3.75 = T-Loss 3.11 + B-Loss 0.64 (train)[0m
[Epoch 004] Total-Loss 4.11 = T-Loss 3.48 + B-Loss 0.63  (val)
training epoch 5
use Semi !!!
[Iteration 000] Total-Loss 3.19 = T-Loss 2.53 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.70 = T-Loss 3.06 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.55 = T-Loss 2.92 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.50 = T-Loss 2.86 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 005] Total-Loss 3.50 = T-Loss 2.86 + B-Loss 0.64 (train)[0m
[Epoch 005] Total-Loss 4.02 = T-Loss 3.38 + B-Loss 0.64  (val)
6
n_iter  0 : loss (0.197945) + tot_loss (0.714101) + tot_loss_crop (0.671494) + loss_clip_order (0.693603) = final_loss = 2.277143
n_iter  1 : loss (0.188400) + tot_loss (0.728322) + tot_loss_crop (0.674973) + loss_clip_order (0.693957) = final_loss = 2.285652
n_iter  2 : loss (0.175016) + tot_loss (0.714768) + tot_loss_crop (0.666131) + loss_clip_order (0.695283) = final_loss = 2.251199
n_iter  3 : loss (0.171989) + tot_loss (0.706055) + tot_loss_crop (0.665819) + loss_clip_order (0.691795) = final_loss = 2.235658
n_iter  4 : loss (0.162352) + tot_loss (0.701337) + tot_loss_crop (0.660382) + loss_clip_order (0.691769) = final_loss = 2.215840
n_iter  5 : loss (0.153179) + tot_loss (0.703967) + tot_loss_crop (0.661209) + loss_clip_order (0.689859) = final_loss = 2.208214
n_iter  6 : loss (0.165324) + tot_loss (0.691164) + tot_loss_crop (0.649760) + loss_clip_order (0.693345) = final_loss = 2.199594
n_iter  7 : loss (0.150376) + tot_loss (0.669673) + tot_loss_crop (0.646042) + loss_clip_order (0.690193) = final_loss = 2.156283
n_iter  8 : loss (0.152627) + tot_loss (0.673224) + tot_loss_crop (0.643312) + loss_clip_order (0.686080) = final_loss = 2.155242
n_iter  9 : loss (0.156240) + tot_loss (0.658074) + tot_loss_crop (0.639678) + loss_clip_order (0.670847) = final_loss = 2.124839
n_iter 10 : loss (0.170603) + tot_loss (0.655710) + tot_loss_crop (0.630342) + loss_clip_order (0.651014) = final_loss = 2.107670
n_iter 11 : loss (0.172378) + tot_loss (0.628811) + tot_loss_crop (0.626891) + loss_clip_order (0.519516) = final_loss = 1.947596
n_iter 12 : loss (0.182728) + tot_loss (0.630152) + tot_loss_crop (0.657754) + loss_clip_order (0.364327) = final_loss = 1.834961
n_iter 13 : loss (0.173463) + tot_loss (0.621141) + tot_loss_crop (0.659438) + loss_clip_order (0.330719) = final_loss = 1.784761
n_iter 14 : loss (0.165202) + tot_loss (0.624431) + tot_loss_crop (0.640883) + loss_clip_order (0.326702) = final_loss = 1.757218
n_iter 15 : loss (0.167982) + tot_loss (0.629191) + tot_loss_crop (0.629102) + loss_clip_order (0.400989) = final_loss = 1.827264
n_iter 16 : loss (0.171329) + tot_loss (0.639844) + tot_loss_crop (0.624633) + loss_clip_order (0.373539) = final_loss = 1.809345
n_iter 17 : loss (0.161640) + tot_loss (0.643021) + tot_loss_crop (0.625052) + loss_clip_order (0.378515) = final_loss = 1.808228
n_iter 18 : loss (0.161471) + tot_loss (0.642709) + tot_loss_crop (0.628061) + loss_clip_order (0.352735) = final_loss = 1.784976
n_iter 19 : loss (0.160682) + tot_loss (0.625928) + tot_loss_crop (0.626375) + loss_clip_order (0.351314) = final_loss = 1.764298
n_iter 20 : loss (0.167896) + tot_loss (0.629810) + tot_loss_crop (0.628269) + loss_clip_order (0.349305) = final_loss = 1.775279
n_iter 21 : loss (0.155018) + tot_loss (0.644937) + tot_loss_crop (0.636226) + loss_clip_order (0.347150) = final_loss = 1.783331
n_iter 22 : loss (0.163102) + tot_loss (0.624630) + tot_loss_crop (0.628018) + loss_clip_order (0.360379) = final_loss = 1.776128
n_iter 23 : loss (0.151625) + tot_loss (0.631003) + tot_loss_crop (0.630941) + loss_clip_order (0.351342) = final_loss = 1.764911
n_iter 24 : loss (0.163833) + tot_loss (0.618578) + tot_loss_crop (0.617665) + loss_clip_order (0.340109) = final_loss = 1.740185
n_iter 25 : loss (0.158029) + tot_loss (0.625417) + tot_loss_crop (0.618119) + loss_clip_order (0.339761) = final_loss = 1.741326
n_iter 26 : loss (0.156830) + tot_loss (0.623048) + tot_loss_crop (0.618901) + loss_clip_order (0.359377) = final_loss = 1.758156
n_iter 27 : loss (0.151624) + tot_loss (0.624457) + tot_loss_crop (0.619673) + loss_clip_order (0.335140) = final_loss = 1.730896
n_iter 28 : loss (0.158921) + tot_loss (0.601361) + tot_loss_crop (0.610940) + loss_clip_order (0.341288) = final_loss = 1.712510
n_iter 29 : loss (0.159980) + tot_loss (0.616521) + tot_loss_crop (0.617728) + loss_clip_order (0.329225) = final_loss = 1.723454
n_iter 30 : loss (0.159425) + tot_loss (0.607781) + tot_loss_crop (0.624314) + loss_clip_order (0.316293) = final_loss = 1.707813
[Pretraining Epoch 006] Total-Loss 0.61 =  F-Loss 0.61 + Clip-Loss 0.32 (train)
n_iter  0 : loss (0.172001) + tot_loss (0.597492) + tot_loss_crop (0.622568) + loss_clip_order (0.332554) = final_loss = 1.724615
n_iter  1 : loss (0.159041) + tot_loss (0.615038) + tot_loss_crop (0.623902) + loss_clip_order (0.348086) = final_loss = 1.746067
n_iter  2 : loss (0.162796) + tot_loss (0.608179) + tot_loss_crop (0.605812) + loss_clip_order (0.337449) = final_loss = 1.714236
n_iter  3 : loss (0.167900) + tot_loss (0.606145) + tot_loss_crop (0.599310) + loss_clip_order (0.354969) = final_loss = 1.728324
n_iter  4 : loss (0.153350) + tot_loss (0.602932) + tot_loss_crop (0.600886) + loss_clip_order (0.376215) = final_loss = 1.733384
n_iter  5 : loss (0.161061) + tot_loss (0.602321) + tot_loss_crop (0.603482) + loss_clip_order (0.338407) = final_loss = 1.705272
n_iter  6 : loss (0.162527) + tot_loss (0.594298) + tot_loss_crop (0.610522) + loss_clip_order (0.332908) = final_loss = 1.700255
n_iter  7 : loss (0.171214) + tot_loss (0.576862) + tot_loss_crop (0.609253) + loss_clip_order (0.321194) = final_loss = 1.678523
n_iter  8 : loss (0.177147) + tot_loss (0.586060) + tot_loss_crop (0.608330) + loss_clip_order (0.338763) = final_loss = 1.710301
n_iter  9 : loss (0.156731) + tot_loss (0.583098) + tot_loss_crop (0.607266) + loss_clip_order (0.329776) = final_loss = 1.676871
n_iter 10 : loss (0.164146) + tot_loss (0.598008) + tot_loss_crop (0.596109) + loss_clip_order (0.328620) = final_loss = 1.686883
n_iter 11 : loss (0.180614) + tot_loss (0.589945) + tot_loss_crop (0.587882) + loss_clip_order (0.363104) = final_loss = 1.721545
n_iter 12 : loss (0.170073) + tot_loss (0.598892) + tot_loss_crop (0.588764) + loss_clip_order (0.335878) = final_loss = 1.693607
n_iter 13 : loss (0.153101) + tot_loss (0.594765) + tot_loss_crop (0.600076) + loss_clip_order (0.320946) = final_loss = 1.668887
n_iter 14 : loss (0.159878) + tot_loss (0.593322) + tot_loss_crop (0.595658) + loss_clip_order (0.315657) = final_loss = 1.664515
n_iter 15 : loss (0.172943) + tot_loss (0.587954) + tot_loss_crop (0.596559) + loss_clip_order (0.352975) = final_loss = 1.710431
n_iter 16 : loss (0.162116) + tot_loss (0.587015) + tot_loss_crop (0.595575) + loss_clip_order (0.312577) = final_loss = 1.657283
n_iter 17 : loss (0.165838) + tot_loss (0.586890) + tot_loss_crop (0.591429) + loss_clip_order (0.318398) = final_loss = 1.662556
n_iter 18 : loss (0.150703) + tot_loss (0.588937) + tot_loss_crop (0.590213) + loss_clip_order (0.317156) = final_loss = 1.647008
n_iter 19 : loss (0.159062) + tot_loss (0.578969) + tot_loss_crop (0.583875) + loss_clip_order (0.329528) = final_loss = 1.651434
n_iter 20 : loss (0.170769) + tot_loss (0.586550) + tot_loss_crop (0.580209) + loss_clip_order (0.339676) = final_loss = 1.677205
n_iter 21 : loss (0.162290) + tot_loss (0.599623) + tot_loss_crop (0.583775) + loss_clip_order (0.322913) = final_loss = 1.668600
n_iter 22 : loss (0.168321) + tot_loss (0.579323) + tot_loss_crop (0.588182) + loss_clip_order (0.329793) = final_loss = 1.665619
n_iter 23 : loss (0.167297) + tot_loss (0.580540) + tot_loss_crop (0.584667) + loss_clip_order (0.313020) = final_loss = 1.645525
n_iter 24 : loss (0.168399) + tot_loss (0.570793) + tot_loss_crop (0.580948) + loss_clip_order (0.315509) = final_loss = 1.635649
n_iter 25 : loss (0.165467) + tot_loss (0.577074) + tot_loss_crop (0.578382) + loss_clip_order (0.307265) = final_loss = 1.628188
n_iter 26 : loss (0.162842) + tot_loss (0.581624) + tot_loss_crop (0.574288) + loss_clip_order (0.320931) = final_loss = 1.639686
n_iter 27 : loss (0.166798) + tot_loss (0.586357) + tot_loss_crop (0.570791) + loss_clip_order (0.320018) = final_loss = 1.643964
n_iter 28 : loss (0.172757) + tot_loss (0.565104) + tot_loss_crop (0.568353) + loss_clip_order (0.332122) = final_loss = 1.638337
n_iter 29 : loss (0.170073) + tot_loss (0.582821) + tot_loss_crop (0.573167) + loss_clip_order (0.316109) = final_loss = 1.642169
n_iter 30 : loss (0.163255) + tot_loss (0.577383) + tot_loss_crop (0.574119) + loss_clip_order (0.306661) = final_loss = 1.621418
[Pretraining Epoch 007] Total-Loss 0.58 =  F-Loss 0.58 + Clip-Loss 0.31 (train)
n_iter  0 : loss (0.165134) + tot_loss (0.569440) + tot_loss_crop (0.581160) + loss_clip_order (0.323951) = final_loss = 1.639685
n_iter  1 : loss (0.171977) + tot_loss (0.587966) + tot_loss_crop (0.573313) + loss_clip_order (0.313202) = final_loss = 1.646459
n_iter  2 : loss (0.170597) + tot_loss (0.579875) + tot_loss_crop (0.565354) + loss_clip_order (0.324421) = final_loss = 1.640247
n_iter  3 : loss (0.164187) + tot_loss (0.574978) + tot_loss_crop (0.561932) + loss_clip_order (0.334728) = final_loss = 1.635825
n_iter  4 : loss (0.154656) + tot_loss (0.571471) + tot_loss_crop (0.563960) + loss_clip_order (0.323582) = final_loss = 1.613669
n_iter  5 : loss (0.169653) + tot_loss (0.572869) + tot_loss_crop (0.560076) + loss_clip_order (0.315090) = final_loss = 1.617688
n_iter  6 : loss (0.168027) + tot_loss (0.566898) + tot_loss_crop (0.561263) + loss_clip_order (0.319253) = final_loss = 1.615440
n_iter  7 : loss (0.155921) + tot_loss (0.551400) + tot_loss_crop (0.565541) + loss_clip_order (0.308336) = final_loss = 1.581197
n_iter  8 : loss (0.169287) + tot_loss (0.560697) + tot_loss_crop (0.563555) + loss_clip_order (0.315400) = final_loss = 1.608938
n_iter  9 : loss (0.151598) + tot_loss (0.557055) + tot_loss_crop (0.564771) + loss_clip_order (0.311775) = final_loss = 1.585199
n_iter 10 : loss (0.168785) + tot_loss (0.569557) + tot_loss_crop (0.555016) + loss_clip_order (0.309754) = final_loss = 1.603113
n_iter 11 : loss (0.166601) + tot_loss (0.559745) + tot_loss_crop (0.549088) + loss_clip_order (0.321098) = final_loss = 1.596532
n_iter 12 : loss (0.169606) + tot_loss (0.568598) + tot_loss_crop (0.549043) + loss_clip_order (0.311639) = final_loss = 1.598886
n_iter 13 : loss (0.166204) + tot_loss (0.565850) + tot_loss_crop (0.550421) + loss_clip_order (0.304077) = final_loss = 1.586552
n_iter 14 : loss (0.163051) + tot_loss (0.566094) + tot_loss_crop (0.554447) + loss_clip_order (0.306698) = final_loss = 1.590291
n_iter 15 : loss (0.162107) + tot_loss (0.562246) + tot_loss_crop (0.552944) + loss_clip_order (0.323876) = final_loss = 1.601174
n_iter 16 : loss (0.169777) + tot_loss (0.561891) + tot_loss_crop (0.543580) + loss_clip_order (0.307088) = final_loss = 1.582336
n_iter 17 : loss (0.162115) + tot_loss (0.560482) + tot_loss_crop (0.543574) + loss_clip_order (0.309363) = final_loss = 1.575534
n_iter 18 : loss (0.167741) + tot_loss (0.560383) + tot_loss_crop (0.542839) + loss_clip_order (0.313894) = final_loss = 1.584856
n_iter 19 : loss (0.156630) + tot_loss (0.548289) + tot_loss_crop (0.540179) + loss_clip_order (0.313014) = final_loss = 1.558111
n_iter 20 : loss (0.181628) + tot_loss (0.554956) + tot_loss_crop (0.536772) + loss_clip_order (0.309473) = final_loss = 1.582829
n_iter 21 : loss (0.166969) + tot_loss (0.569261) + tot_loss_crop (0.543176) + loss_clip_order (0.308053) = final_loss = 1.587458
n_iter 22 : loss (0.169750) + tot_loss (0.551637) + tot_loss_crop (0.539155) + loss_clip_order (0.330078) = final_loss = 1.590620
n_iter 23 : loss (0.158828) + tot_loss (0.554186) + tot_loss_crop (0.536776) + loss_clip_order (0.301139) = final_loss = 1.550929
n_iter 24 : loss (0.157458) + tot_loss (0.544469) + tot_loss_crop (0.538399) + loss_clip_order (0.302410) = final_loss = 1.542737
n_iter 25 : loss (0.159952) + tot_loss (0.549348) + tot_loss_crop (0.534299) + loss_clip_order (0.301898) = final_loss = 1.545498
n_iter 26 : loss (0.154717) + tot_loss (0.551535) + tot_loss_crop (0.535581) + loss_clip_order (0.302753) = final_loss = 1.544587
n_iter 27 : loss (0.160949) + tot_loss (0.554818) + tot_loss_crop (0.531835) + loss_clip_order (0.297207) = final_loss = 1.544808
n_iter 28 : loss (0.169615) + tot_loss (0.534453) + tot_loss_crop (0.525361) + loss_clip_order (0.305253) = final_loss = 1.534682
n_iter 29 : loss (0.160801) + tot_loss (0.552511) + tot_loss_crop (0.533613) + loss_clip_order (0.306773) = final_loss = 1.553698
n_iter 30 : loss (0.173234) + tot_loss (0.549029) + tot_loss_crop (0.525626) + loss_clip_order (0.299618) = final_loss = 1.547506
[Pretraining Epoch 008] Total-Loss 0.55 =  F-Loss 0.55 + Clip-Loss 0.30 (train)
training epoch 6
use Semi !!!
[Iteration 000] Total-Loss 4.09 = T-Loss 3.39 + B-Loss 0.70 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.91 = T-Loss 3.24 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.80 = T-Loss 3.15 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.76 = T-Loss 3.11 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 006] Total-Loss 3.76 = T-Loss 3.11 + B-Loss 0.65 (train)[0m
[Epoch 006] Total-Loss 4.06 = T-Loss 3.43 + B-Loss 0.63  (val)
training epoch 7
use Semi !!!
[Iteration 000] Total-Loss 3.29 = T-Loss 2.62 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.41 = T-Loss 2.78 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.39 = T-Loss 2.76 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.42 = T-Loss 2.79 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 007] Total-Loss 3.42 = T-Loss 2.79 + B-Loss 0.63 (train)[0m
[Epoch 007] Total-Loss 3.95 = T-Loss 3.31 + B-Loss 0.64  (val)
training epoch 8
use Semi !!!
[Iteration 000] Total-Loss 3.05 = T-Loss 2.39 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.20 = T-Loss 2.58 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.20 = T-Loss 2.57 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.29 = T-Loss 2.65 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 008] Total-Loss 3.29 = T-Loss 2.65 + B-Loss 0.63 (train)[0m
[Epoch 008] Total-Loss 3.87 = T-Loss 3.24 + B-Loss 0.63  (val)
training epoch 9
use Semi !!!
[Iteration 000] Total-Loss 2.96 = T-Loss 2.28 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.26 = T-Loss 2.62 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 3.21 = T-Loss 2.58 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 3.20 = T-Loss 2.57 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 009] Total-Loss 3.20 = T-Loss 2.57 + B-Loss 0.63 (train)[0m
[Epoch 009] Total-Loss 3.75 = T-Loss 3.12 + B-Loss 0.63  (val)
training epoch 10
use Semi !!!
[Iteration 000] Total-Loss 2.78 = T-Loss 2.12 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 3.00 = T-Loss 2.37 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.98 = T-Loss 2.36 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.99 = T-Loss 2.36 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 010] Total-Loss 2.99 = T-Loss 2.36 + B-Loss 0.63 (train)[0m
[Epoch 010] Total-Loss 3.59 = T-Loss 2.96 + B-Loss 0.63  (val)
training epoch 11
use Semi !!!
[Iteration 000] Total-Loss 2.68 = T-Loss 2.02 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.87 = T-Loss 2.24 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.87 = T-Loss 2.24 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.89 = T-Loss 2.26 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 011] Total-Loss 2.89 = T-Loss 2.26 + B-Loss 0.63 (train)[0m
[Epoch 011] Total-Loss 3.61 = T-Loss 2.98 + B-Loss 0.63  (val)
training epoch 12
use Semi !!!
[Iteration 000] Total-Loss 2.70 = T-Loss 2.03 + B-Loss 0.67 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.87 = T-Loss 2.25 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.87 = T-Loss 2.24 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.90 = T-Loss 2.27 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 012] Total-Loss 2.90 = T-Loss 2.27 + B-Loss 0.62 (train)[0m
[Epoch 012] Total-Loss 3.62 = T-Loss 3.00 + B-Loss 0.63  (val)
training epoch 13
use Semi !!!
[Iteration 000] Total-Loss 2.76 = T-Loss 2.10 + B-Loss 0.66 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.91 = T-Loss 2.30 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.87 = T-Loss 2.26 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.85 = T-Loss 2.24 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 013] Total-Loss 2.85 = T-Loss 2.24 + B-Loss 0.62 (train)[0m
[Epoch 013] Total-Loss 3.49 = T-Loss 2.86 + B-Loss 0.63  (val)
training epoch 14
use Semi !!!
[Iteration 000] Total-Loss 2.58 = T-Loss 1.93 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.77 = T-Loss 2.16 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.73 = T-Loss 2.12 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.71 = T-Loss 2.10 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 014] Total-Loss 2.71 = T-Loss 2.10 + B-Loss 0.61 (train)[0m
[Epoch 014] Total-Loss 3.43 = T-Loss 2.80 + B-Loss 0.62  (val)
training epoch 15
use Semi !!!
[Iteration 000] Total-Loss 2.40 = T-Loss 1.76 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.63 = T-Loss 2.02 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.60 = T-Loss 2.00 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.60 = T-Loss 2.00 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 015] Total-Loss 2.60 = T-Loss 2.00 + B-Loss 0.61 (train)[0m
[Epoch 015] Total-Loss 3.34 = T-Loss 2.72 + B-Loss 0.62  (val)
training epoch 16
use Semi !!!
[Iteration 000] Total-Loss 2.34 = T-Loss 1.70 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.57 = T-Loss 1.96 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.54 = T-Loss 1.94 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.55 = T-Loss 1.94 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 016] Total-Loss 2.55 = T-Loss 1.94 + B-Loss 0.60 (train)[0m
[Epoch 016] Total-Loss 3.34 = T-Loss 2.71 + B-Loss 0.63  (val)
training epoch 17
use Semi !!!
[Iteration 000] Total-Loss 2.35 = T-Loss 1.72 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.55 = T-Loss 1.94 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.55 = T-Loss 1.94 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.56 = T-Loss 1.95 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 017] Total-Loss 2.56 = T-Loss 1.95 + B-Loss 0.61 (train)[0m
[Epoch 017] Total-Loss 3.54 = T-Loss 2.89 + B-Loss 0.65  (val)
training epoch 18
use Semi !!!
[Iteration 000] Total-Loss 2.66 = T-Loss 1.98 + B-Loss 0.68 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.76 = T-Loss 2.12 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.73 = T-Loss 2.10 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.74 = T-Loss 2.11 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 018] Total-Loss 2.74 = T-Loss 2.11 + B-Loss 0.63 (train)[0m
[Epoch 018] Total-Loss 3.49 = T-Loss 2.87 + B-Loss 0.63  (val)
training epoch 19
use Semi !!!
[Iteration 000] Total-Loss 2.59 = T-Loss 1.95 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.68 = T-Loss 2.08 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.64 = T-Loss 2.03 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.64 = T-Loss 2.03 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 019] Total-Loss 2.64 = T-Loss 2.03 + B-Loss 0.61 (train)[0m
[Epoch 019] Total-Loss 3.34 = T-Loss 2.72 + B-Loss 0.62  (val)
training epoch 20
use Semi !!!
[Iteration 000] Total-Loss 2.35 = T-Loss 1.72 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.63 = T-Loss 2.03 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.58 = T-Loss 1.98 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.57 = T-Loss 1.97 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 020] Total-Loss 2.57 = T-Loss 1.97 + B-Loss 0.60 (train)[0m
[Epoch 020] Total-Loss 3.35 = T-Loss 2.73 + B-Loss 0.63  (val)
training epoch 21
use Semi !!!
[Iteration 000] Total-Loss 2.35 = T-Loss 1.73 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.53 = T-Loss 1.94 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.50 = T-Loss 1.90 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.49 = T-Loss 1.90 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 021] Total-Loss 2.49 = T-Loss 1.90 + B-Loss 0.60 (train)[0m
[Epoch 021] Total-Loss 3.37 = T-Loss 2.73 + B-Loss 0.64  (val)
training epoch 22
use Semi !!!
[Iteration 000] Total-Loss 2.35 = T-Loss 1.71 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.49 = T-Loss 1.89 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.47 = T-Loss 1.88 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.49 = T-Loss 1.89 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 022] Total-Loss 2.49 = T-Loss 1.89 + B-Loss 0.60 (train)[0m
[Epoch 022] Total-Loss 3.37 = T-Loss 2.73 + B-Loss 0.64  (val)
training epoch 23
use Semi !!!
[Iteration 000] Total-Loss 2.37 = T-Loss 1.72 + B-Loss 0.65 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.53 = T-Loss 1.92 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.50 = T-Loss 1.90 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.51 = T-Loss 1.91 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 023] Total-Loss 2.51 = T-Loss 1.91 + B-Loss 0.60 (train)[0m
[Epoch 023] Total-Loss 3.47 = T-Loss 2.84 + B-Loss 0.64  (val)
training epoch 24
use Semi !!!
[Iteration 000] Total-Loss 2.54 = T-Loss 1.91 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.59 = T-Loss 1.99 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.56 = T-Loss 1.96 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.56 = T-Loss 1.96 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 024] Total-Loss 2.56 = T-Loss 1.96 + B-Loss 0.60 (train)[0m
[Epoch 024] Total-Loss 3.47 = T-Loss 2.83 + B-Loss 0.64  (val)
training epoch 25
use Semi !!!
[Iteration 000] Total-Loss 2.47 = T-Loss 1.83 + B-Loss 0.64 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.70 = T-Loss 2.08 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.70 = T-Loss 2.07 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.68 = T-Loss 2.06 + B-Loss 0.62 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 025] Total-Loss 2.68 = T-Loss 2.06 + B-Loss 0.62 (train)[0m
[Epoch 025] Total-Loss 3.48 = T-Loss 2.85 + B-Loss 0.63  (val)
training epoch 26
use Semi !!!
[Iteration 000] Total-Loss 2.41 = T-Loss 1.78 + B-Loss 0.63 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 010] Total-Loss 2.62 = T-Loss 2.01 + B-Loss 0.60 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 020] Total-Loss 2.65 = T-Loss 2.04 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[Iteration 030] Total-Loss 2.70 = T-Loss 2.09 + B-Loss 0.61 + C-Loss 0.00 + C-EMA-Loss 0.00  (train)
[94m[Epoch 026] Total-Loss 2.70 = T-Loss 2.09 + B-Loss 0.61 (train)[0m
[Epoch 026] Total-Loss 3.47 = T-Loss 2.84 + B-Loss 0.63  (val)
Total Time taken for Running 25 epoch is :1057.596625 secs

real	18m10.955s
user	25m42.845s
sys	9m21.616s
Loading validation Video Information ...
  0% 0/4728 [00:00<?, ?it/s] 20% 931/4728 [00:00<00:00, 9305.15it/s] 39% 1862/4728 [00:00<00:00, 8648.87it/s] 58% 2730/4728 [00:00<00:00, 8035.07it/s] 75% 3539/4728 [00:00<00:00, 7590.89it/s] 91% 4302/4728 [00:00<00:00, 7209.43it/s]100% 4728/4728 [00:00<00:00, 7464.04it/s]Inference start

Inference finished
Start Post-Processing
End Post-Processing

real	5m6.615s
user	9m39.333s
sys	1m35.885s
Detection: average-mAP 25.864 mAP@0.50 43.369 mAP@0.55 39.986 mAP@0.60 35.992 mAP@0.65 32.391 mAP@0.70 28.886 mAP@0.75 25.176 mAP@0.80 20.610 mAP@0.85 15.956 mAP@0.90 11.098 mAP@0.95 5.177

real	1m14.569s
user	13m13.961s
sys	0m53.483s

# Early stopping didn't seem to help, despite the suggestion of the train and val accuracies.
